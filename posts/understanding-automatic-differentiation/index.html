<!doctype html><html lang=en dir=ltr class=dark><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Understanding Automatic Differentiation | Bobble Law</title><meta name=generator content="Hugo Eureka 0.9.3"><link rel=stylesheet href=https://bobblelaw.github.io/css/eureka.min.9cec6350e37e534b0338fa9a085bf06855de3b0f2dcf857e792e5e97b07ea905d4d5513db554cbc26a9c3da622bae92d.css><script defer src=https://bobblelaw.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js></script>
<link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload href="https://fonts.googleapis.com/css2?family=Lora:wght@400;600;700&family=Noto+Serif+SC:wght@400;600;700&display=swap" as=style onload='this.onload=null,this.rel="stylesheet"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/base16/solarized-light.min.css media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/cpp,%20swift,%20python.min.js crossorigin></script>
<link rel=stylesheet href=https://bobblelaw.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css media=print onload='this.media="all",this.onload=null'><script defer type=text/javascript src=https://bobblelaw.github.io/js/fontawesome.min.a975d08212c5439f29e6074e7ad58e159ae1ef5efb6a31962fa3b6885557e794dd9315f4a8a16d705066d023f4eaaf07.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ media=print onload='this.media="all",this.onload=null' crossorigin><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><script defer src=https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js integrity=sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0 crossorigin></script>
<link rel=icon type=image/png sizes=32x32 href=https://bobblelaw.github.io/images/icon_hucb7ee3c6385b6f166198d69440e1110c_52330_32x32_fill_box_center_3.png><link rel=apple-touch-icon sizes=180x180 href=https://bobblelaw.github.io/images/icon_hucb7ee3c6385b6f166198d69440e1110c_52330_180x180_fill_box_center_3.png><meta name=description content="My humble understand towards AD."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://bobblelaw.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Understanding Automatic Differentiation","item":"https://bobblelaw.github.io/posts/understanding-automatic-differentiation/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://bobblelaw.github.io/posts/understanding-automatic-differentiation/"},"headline":"Understanding Automatic Differentiation | Bobble Law","datePublished":"2021-03-17T20:47:18+08:00","dateModified":"2022-11-20T22:52:56+08:00","wordCount":2353,"author":{"@type":"Person","name":["host"]},"publisher":{"@type":"Person","name":"Bob Law","logo":{"@type":"ImageObject","url":"https://bobblelaw.github.io/images/icon.png"}},"description":"My humble understand towards AD."}</script><meta property="og:title" content="Understanding Automatic Differentiation | Bobble Law"><meta property="og:type" content="article"><meta property="og:image" content="https://bobblelaw.github.io/images/icon.png"><meta property="og:url" content="https://bobblelaw.github.io/posts/understanding-automatic-differentiation/"><meta property="og:description" content="My humble understand towards AD."><meta property="og:locale" content="en"><meta property="og:site_name" content="Bobble Law"><meta property="article:published_time" content="2021-03-17T20:47:18+08:00"><meta property="article:modified_time" content="2022-11-20T22:52:56+08:00"><meta property="article:section" content="posts"><meta property="article:tag" content="optimization"><body class="flex min-h-screen flex-col"><header class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"><div class="mx-auto w-full max-w-screen-xl"><script>let storageColorScheme=localStorage.getItem("lightDarkMode");(storageColorScheme=="Auto"&&window.matchMedia("(prefers-color-scheme: light)").matches||storageColorScheme=="Light")&&document.getElementsByTagName("html")[0].classList.remove("dark")</script><nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0"><a href=/ class="me-6 text-primary-text text-xl font-bold">Bobble Law</a>
<button id=navbar-btn class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
<i class="fas fa-bars"></i></button><div id=target class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20"><div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0"><a href=/#about class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">About</a>
<a href=/posts/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 selected-menu-item me-4">Posts</a>
<a href=/docs/ class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2 border-transparent me-4">Topics</a></div><div class=flex><div class="relative pt-4 md:pt-0"><div class="cursor-pointer hover:text-eureka" id=lightDarkMode><i class="fas fa-moon"></i></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id=is-open></div><div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40" id=lightDarkOptions><span class="px-4 py-1 hover:text-eureka" name=Light>Light</span>
<span class="px-4 py-1 hover:text-eureka" name=Dark>Dark</span>
<span class="px-4 py-1 hover:text-eureka" name=Auto>Auto</span></div></div></div></div><div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id=is-open-mobile></div></nav><script>let element=document.getElementById("lightDarkMode");storageColorScheme=="Auto"?(element.firstElementChild.classList.remove("fa-moon"),element.firstElementChild.setAttribute("data-icon","adjust"),element.firstElementChild.classList.add("fa-adjust"),document.addEventListener("DOMContentLoaded",()=>{window.matchMedia("(prefers-color-scheme: dark)").addEventListener("change",switchDarkMode)})):storageColorScheme=="Light"&&(element.firstElementChild.classList.remove("fa-moon"),element.firstElementChild.setAttribute("data-icon","sun"),element.firstElementChild.classList.add("fa-sun")),document.addEventListener("DOMContentLoaded",()=>{getcolorscheme(),switchBurger()})</script></div></header><main class="grow pt-16"><div class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl lg:px-4 xl:px-8"><div class="grid grid-cols-2 gap-4 lg:grid-cols-8 lg:pt-12"><div class="lg:col-start-2 bg-secondary-bg col-span-2 rounded px-6 py-8 lg:col-span-6"><article class=prose><h1 class=mb-4>Understanding Automatic Differentiation</h1><div class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"><div class="me-6 my-2"><i class="fas fa-calendar me-1"></i>
<span>2021-03-17</span></div><div class="me-6 my-2"><i class="fas fa-clock me-1"></i>
<span>12 min read</span></div></div><p>Deriving derivatives is not fun. In this post, I will deep dive into the methods for automatic differentiation (AD). After reading this post, I hope you can feel confident with using the various AD techniques, and hopefully never manually calculate derivatives again. Note that this post is not a comparison between AD libraries. For that, a good starting point is <a href>here</a>.</p><h2 id=why-automatic-differentiation>Why Automatic Differentiation?</h2><p>Automatic differentiation is a natural continuation of scientists and engineers’ pursuit for mechanizing computation. After all, we learn how to take derivatives by memorizing a set of rules. Why can’t computers do the same thing?</p><p>Nevertheless, if you just simply follow the rules and symbolically solve for derivatives, the results you get will be deep in the expression hell. Consider the product rule we are all familiar with:</p><div>\[
\begin{align}
\frac{d}{d x}(f(x) g(x)) = \left(\frac{d}{d x} f(x)\right) g(x)+f(x)\left(\frac{d}{d x} g(x)\right)
\end{align}
\]</div><p>Assume that we are not conducting any simplification along the way. By simply following the product rule, we are multiplying the number of common terms in both $f(x)$ and
$\frac{d}{dx}f(x)$, and $g(x)$ and $\frac{d}{dx}g(x)$ by two. Essentially, this well result in a tree-like structure where the number of terms increase exponentially. Table 1 shows the results of applying symbolic differentiation using MATLAB’s diff(f,x) function without simplification. Notice how the number of terms drastically increase, and how there are a repetition of the same terms in the derivatives’ expressions (which will come in handy when we discuss the algorithms for AD).</p><p>So what if we find derivatives numerically? After all, in most applications we don’t care about the forms of the derivatives, only the final values. Perhaps we can try the finite difference method. It is essentially the numerical approximation to the definition of gradients. Given a scalar multivariate function $f: \reals^n \to \reals$ , we can approximate the gradients as</p><div>\[
\begin{align}
\frac{\partial{f(\bold{x})}}{\partial{x}}(f(x) g(x)) \approx \frac{f(\bold{x}+\varDelta_i)-f(x)}{h}
\end{align}
\]</div><p>where $\varDelta_i$ is a small increment on dimension $i$. This is the so-called forward difference method.</p><p>There are a few issues with this method. First, it requires $O(n)$ work for n-dimensional gradients, which is prohibitive for neural networks with millions of learnable parameters. It is also plagued by numerical errors. Specifically, round-off errors (errors caused by using finite-bit floating point representations) and truncation errors (the difference between the analytical gradients and the numerical gradients). At small $\varDelta_i$, round-off errors dominate. At large $\varDelta_i$, truncation errors dominate (see Fig. 2). Such errors might be significant in ill-conditioned problems, causing numerical instabilities.</p><p>Automatic differentiation, on the other hand, is a solution to the problem of calculating derivatives without the downfalls of symbolic differentiation and finite differences. The key idea behind AD is to decompose calculations into elementary steps that form an evaluation trace, and combine each step’s derivative together through the chain rule. Because of the use of evaluation traces, AD can differentiate through not only closed-form calculations, but also control flow statements used in programs. Regardless of the actual path taken, at the end numerical computations will form an evaluation trace which can be used for AD.</p><p>In the rest of this post, I introduce the definition of evaluation traces, and the two modes of AD: forward and reverse.</p><h2 id=evaluation-traces>Evaluation Traces</h2><p>To decompose the functions into elementary steps, we need to construct their <strong>evaluation traces</strong>. You can view these traces as a recording of the steps you take to reach the final results. Let’s take a look at an example function $f: \reals^2 \to \reals^1$:</p><div>\[
\begin{align}
y = \left[\sin \left(x_{1} / x_{2}\right)+x_{1} / x_{2}-\exp \left(x_{2}\right)\right] \left[x_{1} / x_{2}-\exp \left(x_{2}\right)\right]
\end{align}
\]</div><h2 id=forward-mode>Forward Mode</h2><h3 id=accumulating-the-tangent-trace>Accumulating the Tangent Trace</h3><p>Let’s say we want to calculate the partial derivative of $y$ with respect to $x_1$, with $x_1=1.5$ and $x_2=0.5$. As we mentioned above, we can try do it one intermediate variable at a time. Note that we are <strong>only calculating the numerical value</strong> of the derivative. For each $v_i$, we calculate $.{v_i}=\frac{\partial{v_i}}{\partial{x_1}}$. Let’s try a few variables to see how it goes:</p><div>\[
\begin{align}
\dot{v}_{-1} &= \frac{\partial x_{1}}{\partial x_{1}} = 1.0 \\
\dot{v}_{0} &= \frac{\partial x_{2}}{\partial x_{1}} = 0 \\
\dot{v}_{1} &= \frac{\partial (v_{-1} / v_{0}) }{\partial x_{1}} = \dot{v}_{-1} (v_{0}^{-1}) + \dot{v}_{0} (-v_{-1} v_{0}^{-2}) = 1.00 / 0.50 = 2.00 \\
\dot{v}_{2} &= \frac{\partial (\sin{(v_{1})}) }{\partial x_{1}} = \cos(v_{1}) \dot{v_{1}} = -0.99 \times 2.00 = -1.98
\end{align}
\]</div><p>For these calculations, we are simply applying chain rules with basic derivatives. Note that how $.{v_i}$ <strong>only depends on the derivatives and values of the earlier variables</strong>. We can now augment Table 2 to include the derivatives.</p><p>The values of the intermediate variables are sometimes called the <strong>primal trace</strong>, and the derivative values the <strong>tangent trace</strong>.</p><p>This generalizes nicely to a generic vector-valued function $f:\reals^n \to \reals^m$. Assume we are trying to evaluate the function at $\bold{x} = \bold{a}, ~ \bold{a} \in \reals^n$. In that case, the Jacobian matrix is in the form of</p><div>\[
\begin{align}
\mathbf{J}_{f}=\left.\left[\begin{array}{ccc}
\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}} \\
\vdots & \ddots & \vdots \\
    \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
\end{array}\right]\right|_{\mathbf{x}=\mathbf{a}}
\end{align}
\]</div><p>and each column consists of</p><div>\[
\begin{align}
\dot{y}_{j}=\left.\frac{\partial y_{j}}{\partial x_{i}}\right|_{\mathbf{x}=\mathbf{a}}, j=1, \ldots, m
\end{align}
\]</div><p>which are the partial derivatives of $y_i$ with respect to each $x_i$. So we can obtain the columns one by one by setting the corresponding $.{x_i}$ and setting the other entries zero. This opens some interesting techniques. For example, if we want to compute the Jacobian-vector product with $\bold{r}$, we can simply set $\bold{\dot{x}}=\bold{r}$
instead of a unit vector:</p><div>\[
\begin{align}
\mathbf{J}_{f} \mathbf{r}=\left[\begin{array}{ccc}
\frac{\partial y_{1}}{\partial x_{1}} & \cdots & \frac{\partial y_{1}}{\partial x_{n}} \\
\vdots & \ddots & \vdots \\
    \frac{\partial y_{m}}{\partial x_{1}} & \cdots & \frac{\partial y_{m}}{\partial x_{n}}
\end{array}\right]\left[\begin{array}{c}
r_{1} \\
\vdots \\
r_{n}
\end{array}\right].
\end{align}
\]</div><p>In terms of complexity, forward mode is on the order of $O(n)$, where $n$ is the dimension of the input vector. Hence, it will be a great choice for $f: \reals \to \reals^m$, while performing poorly for $f: \reals \to \reals^m$.</p><h3 id=using-dual-numbers>Using Dual Numbers</h3><p>One popular way to implement forward mode AD is to use dual numbers. Dual numbers were introduced by William Clifford in 1873 through his paper <em>Preliminary Sketch of Biquaternions</em>. Clifford was studying how to extend Hamilton’s vectors to consider rotations around not only the origin, but any arbitrary lines in the 3-dimensional space. He called his extension on vectors rotors, and the sum of such rotors motors. He then introduces a symbol $\omega$ to convert motors into vectors, and that $\omega^2=0$. His reasoning was purely geometric:</p><blockquote><p>The symbol $\omega$, applied to any motor, changes it into a vector parallel to its axis and proportional to the rotor part of it. … and if made to operate directly on a vector, reduces it to zero.</p></blockquote><p>However, it turns out that dual numbers can be conveniently to represent differentiation.</p><p>Formally, we represent dual numbers as $a+b\epsilon$. They follow the usual component-wise addition rule:</p><div>\[
\begin{align}
(a + b \epsilon) + (c + d \epsilon) = (a + c) + (b + d) \epsilon.
\end{align}
\]</div><p>And their multiplications work like this (similar to complex numbers):</p><div>\[
\begin{align}
(a+b \epsilon)(c+d \epsilon)=a c+(a d+b c) \epsilon + db \epsilon^{2} = a c + (a d+b c) \epsilon,
\end{align}
\]</div><p>with the rule that $\epsilon^2=0$.</p><p>The connection between dual numbers and differentiation becomes clear once we look at the Taylor series expansion. Given an arbitrary real function $f:\reals \to \reals$, we can express its Taylor series expansion at $x_0$ as</p><div>\[
\begin{align}
f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\cdots+\frac{f^{(n)}\left(x_{0}\right)}{n !}\left(x-x_{0}\right)^{n}+\mathcal{O}\left(\left(x-x_{0}\right)^{n+1}\right)
\end{align}
\]</div><p>as long as the function is $n+1$ times differentiable and has bounded $n+1$ derivative. If we instead extend them to using dual numbers as inputs, it follows that</p><div>\[
\begin{align}
f(a + b \epsilon) =\sum_{n=0}^{\infty} \frac{f^{(n)}(a) b^{n} \epsilon^{n}}{n !}=f(a)+b f^{\prime}(a) \epsilon
\end{align}
\]</div><p>So if we set $b=1$, we can get the derivative at $x=a$ by taking the coefficient of $\epsilon$ after evaluation:</p><div>\[
\begin{align} \label{eq:dual-single-v} \tag{1}
f(a + \epsilon) = f(a) + f^{\prime}(a) \epsilon
\end{align}
\]</div><p>where we expanded the Taylor series at $a$, and since $\epsilon^2=0$, all terms involving $\epsilon^2$ and higher vanish. Under this formulation, addition works:</p><div>\[
\begin{align}
f(a + \epsilon) + g(a + \epsilon) &= f(a) + g(a) + (f^{\prime}(a) + g^{\prime}(a) ) \epsilon.
\end{align}
\]</div><p>Multiplication works:</p><div>\[
\begin{align}
f(a + \epsilon) g(a + \epsilon) &= f(a) g(a) + (f^{\prime}(a) g(a) + f(a) g^{\prime}(a) ) \epsilon.
\end{align}
\]</div><p>And chain rule works as expected:</p><div>\[
\begin{align}
f(g(a + \epsilon)) &=f(g(a) + g^{\prime}(a) \epsilon) \\
&=f(g(a)) + f^{\prime}(g(a)) g^{\prime}(a) \epsilon.
\end{align}
\]</div><p>This essentially gives us the way to conduct forward mode AD by using dual numbers, we can get the primal and tangent trace simultaneously.</p><p>So, how do we take this to higher dimensions?
We simply add an $\epsilon$ for each component.
Assume a function $f: \reals^n \to \reals$.
We define a vector $\boldsymbol{\epsilon} \in \reals^{n}$ where $\boldsymbol{\epsilon}_i^2 = \boldsymbol{\epsilon}_i \ boldsymbol{\epsilon}_j = 0$. If you write out the component-wise computation following Eq.<a href>1</a>, it follows that</p><div>\[
\begin{align}
f(\boldsymbol{a} + \boldsymbol{\epsilon}) = f(\boldsymbol{a}) + \nabla f(\boldsymbol{a}) \cdot \boldsymbol{\epsilon}
\end{align}
\]</div><p>$\nabla f(\boldsymbol{a}) \cdot \boldsymbol{\epsilon}$ can be interpreted as the directional derivative of $f$ in the direction of $\boldsymbol{\epsilon}$. Addition, multiplication and chain rule work as expected:</p><div>\[
\begin{align}
f(\boldsymbol{a} + \boldsymbol{\epsilon}) + g(\boldsymbol{a} + \boldsymbol{\epsilon}) &= f(\boldsymbol{a}) + g(\boldsymbol{a}) + (\nabla f(\boldsymbol{a}) + \nabla g(\boldsymbol{a}) ) \boldsymbol{\epsilon} \\
f(\boldsymbol{a} + \boldsymbol{\epsilon}) g(\boldsymbol{a} + \boldsymbol{\epsilon}) &= f(\boldsymbol{a}) g(\boldsymbol{a}) + (\nabla f(\boldsymbol{a}) g(\boldsymbol{a}) + f(\boldsymbol{a}) \nabla g(\boldsymbol{a}) ) \boldsymbol{\epsilon} \\
f(g(\boldsymbol{a} + \boldsymbol{\epsilon})) &=f(g(\boldsymbol{a})) + \nabla f(g(\boldsymbol{a})) \nabla g(\boldsymbol{a}) \boldsymbol{\epsilon}
\end{align}
\]</div><p>For $f : \reals^n \rightarrow \mathbb{R}^{m}$, instead of using a vector of $\epsilon$, we can use a matrix of $\epsilon$. And for each row of that matrix, we follow the exact same rule as Eq.<a href>2</a>.</p><h2 id=reverse-mode>Reverse Mode</h2><h3 id=propagate-from-the-end>Propagate From the End</h3><p>Reverse mode automatic differentiation, also known as adjoint mode, calculates the derivative by going from the end of the evaluation trace to the beginning. The intuition comes from the chain rule. Consider a function $y = f(x(t))$. From the chain rule, it follows that</p><div>\[
\begin{align}
\frac{\partial y}{\partial t} = \frac{\partial y}{\partial x} \cdot \frac{\partial x}{\partial t}
\end{align}
\]</div><p>The two terms on the right-hand side can be seen as going backwards: $\frac{\partial y}{\partial x}$ can be determined once we calculate $y$ from $x$, and $\frac{\partial x}{\partial t}$ can be calculated once we calculate $x$ from $t$. Extending this to multivariate functions, we have the multivariate chain rule:</p><p>Theorem <a href>1</a>: Suppose $g: \reals^n \to \reals^m$ is differentiable at $a \in \reals^n$ and $f: \reals^m \to \reals^{p}$ is differentiable at $g(a) \in \reals^m$. Then $f \circ g: \reals^n \to \reals^p$ is differentiable at $a$, and its derivative at this point is given by</p><div>\[
D_{a}(f \circ g)=D_{g(a)}(f) D_{a}(g)
\]</div><p>The proof of it can be found in Appendix A4 of <em>Vector Calculus, Linear Algebra, and Differential Forms: a Unified Approach</em> by John H. Hubbard and Barbara Burke Hubbard.</p><p>For example, for a function $F(t) = f(g(t)) = f(x(t), y(t))$ where $f : \reals^{2} \to \reals$ and $g : \reals \to \reals^2$, we have</p><div>\[
\begin{align}
D_{a}(f \circ g)=D_{a}(F)=\frac{d F}{d t} \\
D_{g(a)}(f)=\left[\begin{array}{ll}
\frac{\partial f}{\partial x} & \frac{\partial f}{\partial y}
   \end{array}\right] \\
D_{a}(g)=\left[\begin{array}{l}
\partial x / \partial t \\
\partial y / \partial t
\end{array}\right]
\end{align}
\]</div><p>So it follows</p><div>\[
\begin{align}
\frac{d F}{d t} = \left[\begin{array}{ll}
\frac{\partial f}{\partial x} & \frac{\partial f}{\partial y}
   \end{array}\right]
\left[\begin{array}{l}
\partial x / \partial t \\
\partial y / \partial t
\end{array}\right] = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}
\end{align}
\]</div><p>Applying this idea to automatic differentiation, we have the reverse mode. To calculate derivatives in this mode, we need to conduct two passes. First, we need to do a forward pass, where we obtain the primal trace (Table <a href>2</a>).
We then propagate the partials backward to obtain the desired derivatives (following the chain rule). Let&rsquo;s go back to our example before to see reverse mode in action. Consider again the function</p><div>\[
\begin{align}
y=\left[\sin \left(x_{1} / x_{2}\right)+x_{1} / x_{2}-\exp \left(x_{2}\right)\right] \left[x_{1} / x_{2}-\exp \left(x_{2}\right)\right].
\end{align}
\]</div><p>Following the same way of assigning intermediate variables as in Table <a href>2</a> and in Fig. <a href=#orgc408073>3</a>, we can assemble an adjoint trace. An adjoint (\bar{v}_{i}) is defined as</p><div>\[
\begin{align}
\bar{v}_{i} = \frac{\partial y_{j}}{\partial v_{i}},
\end{align}
\]</div><p>which is equivalent to the product of all the partials from $y_{j}$ up until $v_{i}$. We start from the last variable $v_{6} = y$:</p><div>\[
\begin{align}
\bar{v}_{6} &= \frac{\partial y}{\partial v_{6}} = 1 \\
\bar{v}_{5} &= \frac{\partial y}{\partial v_{5}} = \bar{v}_{6} \frac{\partial v_{6}}{\partial v_{5}} = 1 \times v_{4} = 1.3513 \\
\bar{v}_{4} &= \frac{\partial y}{\partial v_{4}} = \frac{\partial y}{\partial v_{5}} \frac{\partial v_{5}}{\partial v_{4}} + \frac{\partial y}{\partial v_{6}} \frac{\partial v_{6}}{\partial v_{4}} = \bar{v}_{5} \frac{\partial v_{5}}{\partial v_{4}} + \bar{v}_{6} \frac{\partial v_{6}}{\partial v_{4}} = 1.3513 + 1.4914 = 2.8437
\end{align}
\]</div><p>And repeat the same process for all the intermediate variables, we have Table <a href>4</a> below.</p><p>Note how we are able to obtain the adjoints of the input variables $x$ and $y$ (which are equivalent to the partial derivatives of $f$ with respect to $x$ and $y$) at the same time. For a function $f : \reals^n \to \reals$, it takes only one application of reverse mode to compute the entire gradient. In general, if the dimension of the outputs is significantly smaller than that of inputs, reverse mode is a better choice.</p><h2 id=conclusion>Conclusion</h2><p>This post covers basic automatic differentiation techniques for forward and reverse mode. I learned a lot by actually implementing the techniques, instead of just going over the mathematics. For future posts, I might try cover the topics of differentiable programming and optimization for robotics.</p><h2 id=references>References</h2><ul><li>Textbook:<ul><li>Griewank, Andreas, and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2nd ed. Philadelphia, PA: Society for Industrial and Applied Mathematics, 2008.</li></ul></li><li>Papers:<ul><li>Revels, Jarrett, Miles Lubin, and Theodore Papamarkou. “Forward-mode automatic differentiation in Julia.” arXiv preprint arXiv:1607.07892 (2016). (<a href=https://arxiv.org/pdf/1607.07892.pdf>link</a>)</li><li>Baydin, Atilim Gunes, et al. &ldquo;Automatic differentiation in machine learning: a survey.&rdquo; Journal of Marchine Learning Research 18 (2018): 1-43. (<a href=https://www.jmlr.org/papers/volume18/17-468/17-468.pdf>link</a>)</li></ul></li><li>Articles:<ul><li>Reverse-mode automatic differentiation: a tutorial(<a href=https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation>link</a>)</li><li>Engineering Trade-Offs in Automatic Differentiation: from TensorFlow and PyTorch to Jax and Julia(<a href=http://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/>link</a>)</li></ul></li></ul></article><div class=my-4><a href=https://bobblelaw.github.io/tags/optimization/ class="inline-block bg-tertiary-bg text-sm rounded px-3 py-1 my-1 me-2 hover:text-eureka">#optimization</a></div><div class=py-2><div class="my-8 flex flex-col items-center md:flex-row"><a href=https://bobblelaw.github.io/authors/host/ class="md:me-4 text-primary-text h-24 w-24"><img src=https://bobblelaw.github.io/images/cartoon_me.png class="bg-primary-bg w-full rounded-full" alt=Avatar></a><div class="mt-4 w-full md:mt-0 md:w-auto"><a href=https://bobblelaw.github.io/authors/host/ class="mb-2 block border-b pb-1 text-lg font-bold"><h3>Lo, Tszwan</h3></a><span class="block pb-2"></span>
<a href=mailto:bobble2579@hotmail.com class=me-2><i class="fas fa-envelope"></i></a>
<a href=https://github.com/BobbleLaw class=me-2><i class="fab fa-github"></i></a></div></div></div><div class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"><div><span class="text-primary-text block font-bold">Previous</span>
<a href=https://bobblelaw.github.io/posts/understanding-pnp/ class=block>Understanding Perspective-N-Points</a></div><div class="mt-4 md:mt-0 md:text-right"><span class="text-primary-text block font-bold">Next</span>
<a href=https://bobblelaw.github.io/posts/moves-in-return/ class=block>Moves in Returns</a></div></div></div></div><script>document.addEventListener("DOMContentLoaded",()=>{hljs.highlightAll()})</script></div></div></main><footer class=pl-scrollbar><div class="mx-auto w-full max-w-screen-xl"><div class="text-center p-6 pin-b"><p class="text-sm text-tertiary-text">&copy; 2022 <a href=#>Bobble Law</a> and <a href=#>Stay Inc.</a>
&#183; Powered by the <a href=https://github.com/wangchucheng/hugo-eureka class=hover:text-eureka>Eureka</a> theme for <a href=https://gohugo.io class=hover:text-eureka>Hugo</a></p></div></div></footer></body></html>