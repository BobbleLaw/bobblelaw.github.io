<!doctype html><html lang=en><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><meta http-equiv=Accept-CH content="DPR, Viewport-Width, Width"><link rel=icon href=/logo.png type=image/gif><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" media=print onload='this.media="all"'><noscript><link href="https://fonts.googleapis.com/css2?family=Alata&family=Lora:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,600;1,700&family=Roboto:ital,wght@0,100;0,300;0,400;0,500;0,700;0,900;1,100;1,300;1,400;1,500;1,700;1,900&display=swap" rel=stylesheet></noscript><link rel=stylesheet href=/css/font.css media=all><script async src="https://www.googletagmanager.com/gtag/js?id=G-7GMTK23CS7"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7GMTK23CS7")}</script><meta property="og:url" content="https://bobblelaw.github.io/posts/understanding-automatic-differentiation/"><meta property="og:site_name" content="Bobble Law"><meta property="og:title" content="Understanding Automatic Differentiation"><meta property="og:description" content="My humble understand towards AD."><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-03-17T20:47:18+08:00"><meta property="article:modified_time" content="2021-03-17T20:47:18+08:00"><meta property="article:tag" content="Optimization"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Automatic Differentiation"><meta name=twitter:description content="My humble understand towards AD."><script async src="https://www.googletagmanager.com/gtag/js?id=G-7GMTK23CS7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7GMTK23CS7")</script><link rel=stylesheet href=/bootstrap-5/css/bootstrap.min.css media=all><link rel=stylesheet href=/css/header.css media=all><link rel=stylesheet href=/css/footer.css media=all><link rel=stylesheet href=/css/theme.css media=all><style>:root{--text-color:#343a40;--text-secondary-color:#6c757d;--text-link-color:#007bff;--background-color:#eaedf0;--secondary-background-color:#64ffda1a;--primary-color:#007bff;--secondary-color:#f8f9fa;--text-color-dark:#e4e6eb;--text-secondary-color-dark:#b0b3b8;--text-link-color-dark:#ffffff;--background-color-dark:#18191a;--secondary-background-color-dark:#212529;--primary-color-dark:#ffffff;--secondary-color-dark:#212529}body{font-size:1rem;font-weight:400;line-height:1.5;text-align:left}html{background-color:var(--background-color) !important}body::-webkit-scrollbar{height:0;width:8px;background-color:var(--background-color)}::-webkit-scrollbar-track{border-radius:1rem}::-webkit-scrollbar-thumb{border-radius:1rem;background:#b0b0b0;outline:1px solid var(--background-color)}#search-content::-webkit-scrollbar{width:.5em;height:.1em;background-color:var(--background-color)}</style><meta name=description content="My humble understand towards AD."><link rel=stylesheet href=/css/single.css><script defer src=/fontawesome-6/all-6.4.2.js></script><title>Understanding Automatic Differentiation | Bobble Law</title></head><body class=light><script>let localStorageValue=localStorage.getItem("pref-theme"),mediaQuery=window.matchMedia("(prefers-color-scheme: dark)").matches;switch(localStorageValue){case"dark":document.body.classList.add("dark");break;case"light":document.body.classList.remove("dark");break;default:mediaQuery&&document.body.classList.add("dark");break}</script><script>var prevScrollPos=window.pageYOffset;window.addEventListener("scroll",function(){let s=document.getElementById("profileHeader"),t=window.pageYOffset,n=!1,o=!0,i=o?prevScrollPos>t:t>0;i?s.classList.add("showHeaderOnTop"):n=!0,t===0&&(n=!0),n&&s.classList.remove("showHeaderOnTop"),prevScrollPos=t})</script><header id=profileHeader><nav class="pt-3 navbar navbar-expand-lg animate"><div class="container-fluid mx-xs-2 mx-sm-5 mx-md-5 mx-lg-5"><a class="navbar-brand primary-font text-wrap" href=/><img src=/logo.png width=30 height=30 class="d-inline-block align-top">
Bob Law</a><div><input id=search autocomplete=off class="form-control mr-sm-2 d-none d-md-block" placeholder=Search... aria-label=Search oninput=searchOnChange(event)></div><button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarContent aria-controls=navbarContent aria-expanded=false aria-label="Toggle navigation">
<svg aria-hidden="true" height="24" viewBox="0 0 16 16" width="24" data-view-component="true"><path fill-rule="evenodd" d="M1 2.75A.75.75.0 011.75 2h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 2.75zm0 5A.75.75.0 011.75 7h12.5a.75.75.0 110 1.5H1.75A.75.75.0 011 7.75zM1.75 12a.75.75.0 100 1.5h12.5a.75.75.0 100-1.5H1.75z"/></svg></button><div class="collapse navbar-collapse text-wrap primary-font" id=navbarContent><ul class="navbar-nav ms-auto text-center"><li class="nav-item navbar-text d-block d-md-none"><div class=nav-link><input id=search autocomplete=off class="form-control mr-sm-2" placeholder=Search... aria-label=Search oninput=searchOnChange(event)></div></li><li class="nav-item navbar-text"><a class=nav-link href=/#about aria-label=about>About</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#experience aria-label=experience>Experience</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#education aria-label=education>Education</a></li><li class="nav-item navbar-text"><a class=nav-link href=/#projects aria-label=projects>Projects</a></li><li class="nav-item navbar-text"><a class=nav-link href=/posts title>Posts</a></li><li class="nav-item navbar-text"><a class=nav-link href=/tags title>Tags</a></li><li class="nav-item navbar-text"><a class=nav-link href=/topics title>Topics</a></li><li class="nav-item navbar-text"><div class=text-center><button id=theme-toggle>
<svg id="moon" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></li></ul></div></div></nav></header><div id=content><section id=single><div class=container><div class="row justify-content-center"><div class="col-sm-12 col-md-12 col-lg-9"><div class=pr-lg-4><div class="title mb-5"><h1 class="text-center mb-4">Understanding Automatic Differentiation</h1><div class=text-center>Mar 17, 2021
<span id=readingTime>min read</span></div></div><article class="page-content p-2"><p>Deriving derivatives is not fun. In this post, I will deep dive into the methods for automatic differentiation (AD). After reading this post, I hope you can feel confident with using the various AD techniques, and hopefully never manually calculate derivatives again. Note that this post is not a comparison between AD libraries. For that, a good starting point is <a href>here</a>.</p><h2 id=why-automatic-differentiation>Why Automatic Differentiation?</h2><p>Automatic differentiation is a natural continuation of scientists and engineers’ pursuit for mechanizing computation. After all, we learn how to take derivatives by memorizing a set of rules. Why can’t computers do the same thing?</p><p>Nevertheless, if you just simply follow the rules and symbolically solve for derivatives, the results you get will be deep in the expression hell. Consider the product rule we are all familiar with:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \frac{d}{d x}(f(x) g(x)) = \left(\frac{d}{d x} f(x)\right) g(x)+f(x)\left(\frac{d}{d x} g(x)\right)
\end{align}
</code></pre><p>Assume that we are not conducting any simplification along the way. By simply following the product rule, we are multiplying the number of common terms in both $f(x)$ and<br>$\frac{d}{dx}f(x)$, and $g(x)$ and $\frac{d}{dx}g(x)$ by two. Essentially, this well result in a tree-like structure where the number of terms increase exponentially. Table 1 shows the results of applying symbolic differentiation using MATLAB’s diff(f,x) function without simplification. Notice how the number of terms drastically increase, and how there are a repetition of the same terms in the derivatives’ expressions (which will come in handy when we discuss the algorithms for AD).</p><p>So what if we find derivatives numerically? After all, in most applications we don’t care about the forms of the derivatives, only the final values. Perhaps we can try the finite difference method. It is essentially the numerical approximation to the definition of gradients. Given a scalar multivariate function $f: \reals^n \to \reals$ , we can approximate the gradients as</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \frac{\partial{f(\bold{x})}}{\partial{x}}(f(x) g(x)) \approx \frac{f(\bold{x}+\varDelta_i)-f(x)}{h}
\end{align}
</code></pre><p>where $\varDelta_i$ is a small increment on dimension $i$. This is the so-called forward difference method.</p><p>There are a few issues with this method. First, it requires $O(n)$ work for n-dimensional gradients, which is prohibitive for neural networks with millions of learnable parameters. It is also plagued by numerical errors. Specifically, round-off errors (errors caused by using finite-bit floating point representations) and truncation errors (the difference between the analytical gradients and the numerical gradients). At small $\varDelta_i$, round-off errors dominate. At large $\varDelta_i$, truncation errors dominate (see Fig. 2). Such errors might be significant in ill-conditioned problems, causing numerical instabilities.</p><p>Automatic differentiation, on the other hand, is a solution to the problem of calculating derivatives without the downfalls of symbolic differentiation and finite differences. The key idea behind AD is to decompose calculations into elementary steps that form an evaluation trace, and combine each step’s derivative together through the chain rule. Because of the use of evaluation traces, AD can differentiate through not only closed-form calculations, but also control flow statements used in programs. Regardless of the actual path taken, at the end numerical computations will form an evaluation trace which can be used for AD.</p><p>In the rest of this post, I introduce the definition of evaluation traces, and the two modes of AD: forward and reverse.</p><h2 id=evaluation-traces>Evaluation Traces</h2><p>To decompose the functions into elementary steps, we need to construct their <strong>evaluation traces</strong>. You can view these traces as a recording of the steps you take to reach the final results. Let’s take a look at an example function $f: \reals^2 \to \reals^1$:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  y = \left[\sin \left(x_{1} / x_{2}\right)+x_{1} / x_{2}-\exp \left(x_{2}\right)\right] \left[x_{1} / x_{2}-\exp \left(x_{2}\right)\right]
\end{align}
</code></pre><h2 id=forward-mode>Forward Mode</h2><h3 id=accumulating-the-tangent-trace>Accumulating the Tangent Trace</h3><p>Let’s say we want to calculate the partial derivative of $y$ with respect to $x_1$, with $x_1=1.5$ and $x_2=0.5$. As we mentioned above, we can try do it one intermediate variable at a time. Note that we are <strong>only calculating the numerical value</strong> of the derivative. For each $v_i$, we calculate $.{v_i}=\frac{\partial{v_i}}{\partial{x_1}}$. Let’s try a few variables to see how it goes:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \dot{v}_{-1} &amp;= \frac{\partial x_{1}}{\partial x_{1}} = 1.0 \\
  \dot{v}_{0} &amp;= \frac{\partial x_{2}}{\partial x_{1}} = 0 \\
  \dot{v}_{1} &amp;= \frac{\partial (v_{-1} / v_{0}) }{\partial x_{1}} = \dot{v}_{-1} (v_{0}^{-1}) + \dot{v}_{0} (-v_{-1} v_{0}^{-2})  = 1.00 / 0.50 = 2.00 \\
  \dot{v}_{2} &amp;= \frac{\partial (\sin{(v_{1})}) }{\partial x_{1}} = \cos(v_{1}) \dot{v_{1}} = -0.99 \times 2.00 = -1.98
\end{align}
</code></pre><p>For these calculations, we are simply applying chain rules with basic derivatives. Note that how $.{v_i}$ <strong>only depends on the derivatives and values of the earlier variables</strong>. We can now augment Table 2 to include the derivatives.</p><p>The values of the intermediate variables are sometimes called the <strong>primal trace</strong>, and the derivative values the <strong>tangent trace</strong>.</p><p>This generalizes nicely to a generic vector-valued function $f:\reals^n \to \reals^m$. Assume we are trying to evaluate the function at $\bold{x} = \bold{a}, ~ \bold{a} \in \reals^n$. In that case, the Jacobian matrix is in the form of</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \mathbf{J}_{f}=\left.\left[\begin{array}{ccc}
  \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}} \\
  \vdots &amp; \ddots &amp; \vdots \\
  \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
  \end{array}\right]\right|_{\mathbf{x}=\mathbf{a}}
\end{align}
</code></pre><p>and each column consists of</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \dot{y}_{j}=\left.\frac{\partial y_{j}}{\partial x_{i}}\right|_{\mathbf{x}=\mathbf{a}}, j=1, \ldots, m
\end{align}
</code></pre><p>which are the partial derivatives of $y_i$ with respect to each $x_i$. So we can obtain the columns one by one by setting the corresponding $.{x_i}$ and setting the other entries zero. This opens some interesting techniques. For example, if we want to compute the Jacobian-vector product with $\bold{r}$, we can simply set $\bold{\dot{x}}=\bold{r}$<br>instead of a unit vector:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
    \mathbf{J}_{f} \mathbf{r}=\left[\begin{array}{ccc}
    \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}} \\
    \vdots &amp; \ddots &amp; \vdots \\
    \frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}}
    \end{array}\right]\left[\begin{array}{c}
    r_{1} \\
    \vdots \\
    r_{n}
    \end{array}\right].
\end{align}
</code></pre><p>In terms of complexity, forward mode is on the order of $O(n)$, where $n$ is the dimension of the input vector. Hence, it will be a great choice for $f: \reals \to \reals^m$, while performing poorly for $f: \reals \to \reals^m$.</p><h3 id=using-dual-numbers>Using Dual Numbers</h3><p>One popular way to implement forward mode AD is to use dual numbers. Dual numbers were introduced by William Clifford in 1873 through his paper <em>Preliminary Sketch of Biquaternions</em>. Clifford was studying how to extend Hamilton’s vectors to consider rotations around not only the origin, but any arbitrary lines in the 3-dimensional space. He called his extension on vectors rotors, and the sum of such rotors motors. He then introduces a symbol $\omega$ to convert motors into vectors, and that $\omega^2=0$. His reasoning was purely geometric:</p><blockquote><p>The symbol $\omega$, applied to any motor, changes it into a vector parallel to its axis and proportional to the rotor part of it. … and if made to operate directly on a vector, reduces it to zero.</p></blockquote><p>However, it turns out that dual numbers can be conveniently to represent differentiation.</p><p>Formally, we represent dual numbers as $a+b\epsilon$. They follow the usual component-wise addition rule:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  (a + b \epsilon) + (c + d \epsilon) = (a + c) + (b + d) \epsilon.
\end{align}
</code></pre><p>And their multiplications work like this (similar to complex numbers):</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  (a+b \epsilon)(c+d \epsilon)=a c+(a d+b c) \epsilon + db \epsilon^{2} = a c + (a d+b c) \epsilon,
\end{align}
</code></pre><p>with the rule that $\epsilon^2=0$.</p><p>The connection between dual numbers and differentiation becomes clear once we look at the Taylor series expansion. Given an arbitrary real function $f:\reals \to \reals$, we can express its Taylor series expansion at $x_0$ as</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(x)=f\left(x_{0}\right)+f^{\prime}\left(x_{0}\right)\left(x-x_{0}\right)+\cdots+\frac{f^{(n)}\left(x_{0}\right)}{n !}\left(x-x_{0}\right)^{n}+\mathcal{O}\left(\left(x-x_{0}\right)^{n+1}\right)
\end{align}
</code></pre><p>as long as the function is $n+1$ times differentiable and has bounded $n+1$ derivative. If we instead extend them to using dual numbers as inputs, it follows that</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(a + b \epsilon) =\sum_{n=0}^{\infty} \frac{f^{(n)}(a) b^{n} \epsilon^{n}}{n !}=f(a)+b f^{\prime}(a) \epsilon
\end{align}
</code></pre><p>So if we set $b=1$, we can get the derivative at $x=a$ by taking the coefficient of $\epsilon$ after evaluation:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align} \label{eq:dual-single-v} \tag{1}
  f(a + \epsilon) = f(a) + f^{\prime}(a) \epsilon
\end{align}
</code></pre><p>where we expanded the Taylor series at $a$, and since $\epsilon^2=0$, all terms involving $\epsilon^2$ and higher vanish. Under this formulation, addition works:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(a + \epsilon) + g(a + \epsilon) &amp;= f(a) + g(a) + (f^{\prime}(a) + g^{\prime}(a) ) \epsilon.
\end{align}
</code></pre><p>Multiplication works:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(a + \epsilon)  g(a + \epsilon) &amp;= f(a) g(a) + (f^{\prime}(a) g(a)  + f(a) g^{\prime}(a) ) \epsilon.
\end{align}
</code></pre><p>And chain rule works as expected:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(g(a + \epsilon)) &amp;=f(g(a) + g^{\prime}(a) \epsilon) \\
  &amp;=f(g(a)) + f^{\prime}(g(a)) g^{\prime}(a) \epsilon.
\end{align}
</code></pre><p>This essentially gives us the way to conduct forward mode AD by using dual numbers, we can get the primal and tangent trace simultaneously.</p><p>So, how do we take this to higher dimensions?<br>We simply add an $\epsilon$ for each component.<br>Assume a function $f: \reals^n \to \reals$.<br>We define a vector $\boldsymbol{\epsilon} \in \reals^{n}$ where $\boldsymbol{\epsilon}_i^2 = \boldsymbol{\epsilon}_i \ boldsymbol{\epsilon}_j = 0$. If you write out the component-wise computation following Eq.<a href>1</a>, it follows that</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(\boldsymbol{a} + \boldsymbol{\epsilon}) = f(\boldsymbol{a}) + \nabla f(\boldsymbol{a}) \cdot \boldsymbol{\epsilon}
\end{align}
</code></pre><p>$\nabla f(\boldsymbol{a}) \cdot \boldsymbol{\epsilon}$ can be interpreted as the directional derivative of $f$ in the direction of $\boldsymbol{\epsilon}$. Addition, multiplication and chain rule work as expected:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  f(\boldsymbol{a} + \boldsymbol{\epsilon}) + g(\boldsymbol{a} + \boldsymbol{\epsilon}) &amp;= f(\boldsymbol{a}) + g(\boldsymbol{a}) + (\nabla f(\boldsymbol{a}) + \nabla g(\boldsymbol{a}) ) \boldsymbol{\epsilon} \\
  f(\boldsymbol{a} + \boldsymbol{\epsilon})   g(\boldsymbol{a} + \boldsymbol{\epsilon}) &amp;= f(\boldsymbol{a}) g(\boldsymbol{a}) + (\nabla f(\boldsymbol{a}) g(\boldsymbol{a})  + f(\boldsymbol{a}) \nabla g(\boldsymbol{a}) ) \boldsymbol{\epsilon} \\
  f(g(\boldsymbol{a} + \boldsymbol{\epsilon})) &amp;=f(g(\boldsymbol{a})) + \nabla f(g(\boldsymbol{a})) \nabla g(\boldsymbol{a}) \boldsymbol{\epsilon}
\end{align}
</code></pre><p>For $f : \reals^n \rightarrow \mathbb{R}^{m}$, instead of using a vector of $\epsilon$, we can use a matrix of $\epsilon$. And for each row of that matrix, we follow the exact same rule as Eq.<a href>2</a>.</p><h2 id=reverse-mode>Reverse Mode</h2><h3 id=propagate-from-the-end>Propagate From the End</h3><p>Reverse mode automatic differentiation, also known as adjoint mode, calculates the derivative by going from the end of the evaluation trace to the beginning. The intuition comes from the chain rule. Consider a function $y = f(x(t))$. From the chain rule, it follows that</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \frac{\partial y}{\partial t} = \frac{\partial y}{\partial x} \cdot \frac{\partial x}{\partial t}
\end{align}
</code></pre><p>The two terms on the right-hand side can be seen as going backwards: $\frac{\partial y}{\partial x}$ can be determined once we calculate $y$ from $x$, and $\frac{\partial x}{\partial t}$ can be calculated once we calculate $x$ from $t$. Extending this to multivariate functions, we have the multivariate chain rule:</p><p>Theorem <a href>1</a>: Suppose $g: \reals^n \to \reals^m$ is differentiable at $a \in \reals^n$ and $f: \reals^m \to \reals^{p}$ is differentiable at $g(a) \in \reals^m$. Then $f \circ g: \reals^n \to \reals^p$ is differentiable at $a$, and its derivative at this point is given by</p><pre tabindex=0><code class=language-math data-lang=math>D_{a}(f \circ g)=D_{g(a)}(f) D_{a}(g)
</code></pre><p>The proof of it can be found in Appendix A4 of <em>Vector Calculus, Linear Algebra, and Differential Forms: a Unified Approach</em> by John H. Hubbard and Barbara Burke Hubbard.</p><p>For example, for a function $F(t) = f(g(t)) = f(x(t), y(t))$ where $f : \reals^{2} \to \reals$ and $g : \reals \to \reals^2$, we have</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  D_{a}(f \circ g)=D_{a}(F)=\frac{d F}{d t} \\
  D_{g(a)}(f)=\left[\begin{array}{ll}
    \frac{\partial f}{\partial x} &amp; \frac{\partial f}{\partial y}
    \end{array}\right] \\
  D_{a}(g)=\left[\begin{array}{l}
    \partial x / \partial t \\
    \partial y / \partial t
    \end{array}\right]
\end{align}
</code></pre><p>So it follows</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \frac{d F}{d t} = \left[\begin{array}{ll}
    \frac{\partial f}{\partial x} &amp; \frac{\partial f}{\partial y}
    \end{array}\right]
    \left[\begin{array}{l}
    \partial x / \partial t \\
    \partial y / \partial t
    \end{array}\right] = \frac{\partial f}{\partial x} \frac{\partial x}{\partial t} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial t}
\end{align}
</code></pre><p>Applying this idea to automatic differentiation, we have the reverse mode. To calculate derivatives in this mode, we need to conduct two passes. First, we need to do a forward pass, where we obtain the primal trace (Table <a href>2</a>).<br>We then propagate the partials backward to obtain the desired derivatives (following the chain rule). Let&rsquo;s go back to our example before to see reverse mode in action. Consider again the function</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  y=\left[\sin \left(x_{1} / x_{2}\right)+x_{1} / x_{2}-\exp \left(x_{2}\right)\right] \left[x_{1} / x_{2}-\exp \left(x_{2}\right)\right].
\end{align}
</code></pre><p>Following the same way of assigning intermediate variables as in Table <a href>2</a> and in Fig. <a href=#orgc408073>3</a>, we can assemble an adjoint trace. An adjoint (\bar{v}_{i}) is defined as</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \bar{v}_{i} = \frac{\partial y_{j}}{\partial v_{i}},
\end{align}
</code></pre><p>which is equivalent to the product of all the partials from $y_{j}$ up until $v_{i}$. We start from the last variable $v_{6} = y$:</p><pre tabindex=0><code class=language-math data-lang=math>\begin{align}
  \bar{v}_{6} &amp;= \frac{\partial y}{\partial v_{6}} = 1 \\
  \bar{v}_{5} &amp;= \frac{\partial y}{\partial v_{5}} = \bar{v}_{6} \frac{\partial v_{6}}{\partial v_{5}} = 1 \times v_{4} = 1.3513 \\
  \bar{v}_{4} &amp;= \frac{\partial y}{\partial v_{4}} = \frac{\partial y}{\partial v_{5}} \frac{\partial v_{5}}{\partial v_{4}} + \frac{\partial y}{\partial v_{6}} \frac{\partial v_{6}}{\partial v_{4}} = \bar{v}_{5} \frac{\partial v_{5}}{\partial v_{4}} + \bar{v}_{6} \frac{\partial v_{6}}{\partial v_{4}} = 1.3513 + 1.4914 = 2.8437
\end{align}
</code></pre><p>And repeat the same process for all the intermediate variables, we have Table <a href>4</a> below.</p><p>Note how we are able to obtain the adjoints of the input variables $x$ and $y$ (which are equivalent to the partial derivatives of $f$ with respect to $x$ and $y$) at the same time. For a function $f : \reals^n \to \reals$, it takes only one application of reverse mode to compute the entire gradient. In general, if the dimension of the outputs is significantly smaller than that of inputs, reverse mode is a better choice.</p><h2 id=conclusion>Conclusion</h2><p>This post covers basic automatic differentiation techniques for forward and reverse mode. I learned a lot by actually implementing the techniques, instead of just going over the mathematics. For future posts, I might try cover the topics of differentiable programming and optimization for robotics.</p><h2 id=references>References</h2><ul><li>Textbook:<ul><li>Griewank, Andreas, and Andrea Walther. Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation. 2nd ed. Philadelphia, PA: Society for Industrial and Applied Mathematics, 2008.</li></ul></li><li>Papers:<ul><li>Revels, Jarrett, Miles Lubin, and Theodore Papamarkou. “Forward-mode automatic differentiation in Julia.” arXiv preprint arXiv:1607.07892 (2016). (<a href=https://arxiv.org/pdf/1607.07892.pdf>link</a>)</li><li>Baydin, Atilim Gunes, et al. &ldquo;Automatic differentiation in machine learning: a survey.&rdquo; Journal of Marchine Learning Research 18 (2018): 1-43. (<a href=https://www.jmlr.org/papers/volume18/17-468/17-468.pdf>link</a>)</li></ul></li><li>Articles:<ul><li>Reverse-mode automatic differentiation: a tutorial(<a href=https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation>link</a>)</li><li>Engineering Trade-Offs in Automatic Differentiation: from TensorFlow and PyTorch to Jax and Julia(<a href=http://www.stochasticlifestyle.com/engineering-trade-offs-in-automatic-differentiation-from-tensorflow-and-pytorch-to-jax-and-julia/>link</a>)</li></ul></li></ul></article></div></div><div class="col-sm-12 col-md-12 col-lg-3"><div id=stickySideBar class=sticky-sidebar><aside class=tags><h5>Tags</h5><ul class="tags-ul list-unstyled list-inline"><li class=list-inline-item><a href=https://bobblelaw.github.io/tags/optimization target=_blank>Optimization</a></li></ul></aside><aside class=social><h5>Social</h5><div class=social-content><ul class=list-inline><li class="list-inline-item text-center"><a target=_blank href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fbobblelaw.github.io%2fposts%2funderstanding-automatic-differentiation%2f"><i class="fab fa-linkedin"></i></a></li><li class="list-inline-item text-center"><a target=_blank href="https://twitter.com/share?text=Understanding%20Automatic%20Differentiation&url=https%3a%2f%2fbobblelaw.github.io%2fposts%2funderstanding-automatic-differentiation%2f"><i class="fab fa-twitter"></i></a></li><li class="list-inline-item text-center"><a target=_blank href="https://api.whatsapp.com/send?text=Understanding%20Automatic%20Differentiation: https%3a%2f%2fbobblelaw.github.io%2fposts%2funderstanding-automatic-differentiation%2f"><i class="fab fa-whatsapp"></i></a></li><li class="list-inline-item text-center"><a target=_blank href='mailto:?subject=Understanding%20Automatic%20Differentiation&amp;body=Check%20out%20this%20site https%3a%2f%2fbobblelaw.github.io%2fposts%2funderstanding-automatic-differentiation%2f'><i class="fa fa-envelope"></i></a></li></ul></div></aside></div></div></div><div class=row><div class="col-sm-12 col-md-12 col-lg-9 p-4"></div></div></div><button class="p-2 px-3" onclick=topFunction() id=topScroll>
<i class="fas fa-angle-up"></i></button></section><div class=progress><div id=scroll-progress-bar class=progress-bar role=progressbar aria-valuenow=0 aria-valuemin=0 aria-valuemax=100></div></div><script src=/js/scrollProgressBar.js></script><script>var topScroll=document.getElementById("topScroll");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?topScroll.style.display="block":topScroll.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}let stickySideBarElem=document.getElementById("stickySideBar"),stickyNavBar=!0;if(stickyNavBar){let e=document.getElementById("profileHeader"),t=e.offsetHeight+15;stickySideBarElem.style.top=t+"px"}else stickySideBarElem.style.top="50px"</script><script src=/js/readingTime.js></script></div><footer><div class="text-center pt-2"><span class=px-1><a href=https://github.com/BobbleLaw aria-label=github><svg width="2.7em" height="2.7em" viewBox="0 0 1792 1792"><path id="footer-socialNetworks-github-svg-path" d="M522 1352q-8 9-20-3-13-11-4-19 8-9 20 3 12 11 4 19zm-42-61q9 12 0 19-8 6-17-7t0-18q9-7 17 6zm-61-60q-5 7-13 2-10-5-7-12 3-5 13-2 10 5 7 12zm31 34q-6 7-16-3-9-11-2-16 6-6 16 3 9 11 2 16zm129 112q-4 12-19 6-17-4-13-15t19-7q16 5 13 16zm63 5q0 11-16 11-17 2-17-11 0-11 16-11 17-2 17 11zm58-10q2 10-14 14t-18-8 14-15q16-2 18 9zm964-956v960q0 119-84.5 203.5T1376 1664h-224q-16 0-24.5-1t-19.5-5-16-14.5-5-27.5v-239q0-97-52-142 57-6 102.5-18t94-39 81-66.5 53-105T1386 856q0-121-79-206 37-91-8-204-28-9-81 11t-92 44l-38 24q-93-26-192-26t-192 26q-16-11-42.5-27T578 459.5 492 446q-44 113-7 204-79 85-79 206 0 85 20.5 150t52.5 105 80.5 67 94 39 102.5 18q-40 36-49 103-21 10-45 15t-57 5-65.5-21.5T484 1274q-19-32-48.5-52t-49.5-24l-20-3q-21 0-29 4.5t-5 11.5 9 14 13 12l7 5q22 10 43.5 38t31.5 51l10 23q13 38 44 61.5t67 30 69.5 7 55.5-3.5l23-4q0 38 .5 103t.5 68q0 22-11 33.5t-22 13-33 1.5H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z"/></svg>
</a></span><span class=px-1><a href=https://www.linkedin.com/in/boblzy aria-label=linkedin><svg width="2.4em" height="2.4em" fill="#fff" aria-label="LinkedIn" viewBox="0 0 512 512"><rect width="512" height="512" fill="#0077b5" rx="15%"/><circle cx="142" cy="138" r="37"/><path stroke="#fff" stroke-width="66" d="M244 194v198M142 194v198"/><path d="M276 282c0-20 13-40 36-40 24 0 33 18 33 45v105h66V279c0-61-32-89-76-89-34 0-51 19-59 32"/></svg></a></span></div><div class="container py-4"><div class="row justify-content-center"><div class="col-md-4 text-center"><div class=pb-2><a href=https://bobblelaw.github.io/ title="Bobble Law"><img alt="Footer logo" src=/logo.png height=40px width=40px></a></div>&copy; 2025 All rights reserved<div class=text-secondary>Made with
<span class=text-danger>&#10084;
</span>and
<a href=https://github.com/gurusabarish/hugo-profile target=_blank title="Designed and developed by gurusabarish">Hugo Profile</a></div></div></div></div></footer><script src=/bootstrap-5/js/bootstrap.bundle.min.js></script><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))});var tooltipTriggerList=[].slice.call(document.querySelectorAll('[data-bs-toggle="tooltip"]')),tooltipList=tooltipTriggerList.map(function(e){return new bootstrap.Tooltip(e)})</script><script src=/js/search.js></script><section id=search-content class=py-2><div class=container id=search-results></div></section></body></html>