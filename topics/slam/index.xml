<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Bobble Law</title><link>https://bobblelaw.github.io/topics/slam/</link><description>Recent content on Bobble Law</description><generator>Hugo</generator><language>en-US</language><atom:link href="https://bobblelaw.github.io/topics/slam/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://bobblelaw.github.io/topics/slam/msckf/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bobblelaw.github.io/topics/slam/msckf/</guid><description>&lt;h2 id="notations">Notations&lt;/h2>
&lt;h2 id="review-of-extended-kalman-filter">Review of Extended Kalman Filter&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
x_{k} &amp;amp;= f(x_{k-1}, u_{k-1}) + w_{k-1} \\
z_{k} &amp;amp;= h(x_{k}) + v_{k-1}
\end{align}
&lt;/code>&lt;/pre>&lt;p>where $w_{k} ∼ N(0, Q_{k})$, $v_{k} ∼ N(0, R_{k})$.&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
_{G}^{B}\.{q}(t) &amp;amp;= \frac{1}{2} Ω(^{G}ω(t))_{G}^{B}q(t), ~ 
with ~ Ω(ω)= \begin{bmatrix}
 -[ω]_{×} &amp;amp; ω \\
 -ω^{T} &amp;amp; 0
 \end{bmatrix} \\
^{G}\.{p}(t) &amp;amp;= ~ ^{G}v(t) \\
^{G}\.{v}(t) &amp;amp;= ~ ^{G}a(t) \\
\.{b}_{g}(t) &amp;amp;= n_{w_{g}}(t) \\
\.{b}_{a}(t) &amp;amp;= n_{w_{a}}(t)
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
_{G}^{B}\.{q}(t) &amp;amp;= \frac{1}{2} Ω(^{G}ω(t))_{G}^{B}q(t), ~ 
with ~ Ω(ω)= \begin{bmatrix}
 -[ω]_{×} &amp;amp; ω \\
 -ω^{T} &amp;amp; 0
 \end{bmatrix} \\
^{G}\.{p}(t) &amp;amp;= ~ ^{G}v(t) \\
^{G}\.{v}(t) &amp;amp;= ~ ^{G}a(t) \\
\.{b}_{g}(t) &amp;amp;= n_{w_{g}}(t) \\
\.{b}_{a}(t) &amp;amp;= n_{w_{a}}(t)
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\bold{x}_{B} = [^{G}q_{B}^{T},~ ^{G}p_{B}^{T},~ ^{G}v_{B}^{T},~ b_{g}^{T},~ b_{a}^{T}]^{T}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\~{\bold{x}}_{B} = [^{G}\delta\theta_{B}^{T},~ ^{G}\~p_{B}^{T},~ ^{G}\~v_{B}^{T},~ \~b_{g}^{T},~ \~b_{a}^{T}]^{T}
&lt;/code>&lt;/pre>&lt;h2 id="complete-state-vector">Complete state vector&lt;/h2>
&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{pmatrix} a&amp;amp;b\\c&amp;amp;d \end{pmatrix} \quad
\begin{bmatrix} a&amp;amp;b\\c&amp;amp;d \end{bmatrix} \quad
\begin{Bmatrix} a&amp;amp;b\\c&amp;amp;d \end{Bmatrix} \quad
\begin{vmatrix} a&amp;amp;b\\c&amp;amp;d \end{vmatrix} 
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{aligned}
x ={}&amp;amp; a+b+c+{} \\
&amp;amp;d+e+f+g
\end{aligned}
&lt;/code>&lt;/pre></description></item><item><title/><link>https://bobblelaw.github.io/topics/slam/vins/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bobblelaw.github.io/topics/slam/vins/</guid><description>&lt;p>&lt;em>WIP&lt;/em>&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
_{G}^{B}\.{q}(t) &amp;amp;= \frac{1}{2} Ω(^{G}ω(t))_{G}^{B}q(t), ~ 
with ~ Ω(ω)= \begin{bmatrix}
 -[ω]_{×} &amp;amp; ω \\
 -ω^{T} &amp;amp; 0
 \end{bmatrix} \\
^{G}\.{p}(t) &amp;amp;= ~ ^{G}v(t) \\
^{G}\.{v}(t) &amp;amp;= ~ ^{G}a(t) \\
\.{b}_{g}(t) &amp;amp;= n_{w_{g}}(t) \\
\.{b}_{a}(t) &amp;amp;= n_{w_{a}}(t)
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\int u \frac{dv}{dx}\, dx=uv-\int \frac{du}{dx}v\,dx
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">p(\boldsymbol{x} | \boldsymbol{z}) = p(\boldsymbol{z} | \boldsymbol{x})p(\boldsymbol{x})
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">p(\boldsymbol{z} | \boldsymbol{x}) = \prod_{i} p(\boldsymbol{z_i} | \boldsymbol{x_i})
&lt;/code>&lt;/pre>&lt;p>For example,&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
 \~a_t &amp;amp;= a_t + ~ _w^T{R} ~ ^w{g} + b_{a_t} + n_a \\
 \~\omega &amp;amp;= \omega_t + b_{\omega_t} + n_\omega
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">n_a \sim N(0, \sigma_a^2) \\
n_{\omega} \sim N(0, \sigma_{\omega}^2)
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
 _{B_{k+1}}^G{p} &amp;amp;=~ _{B_{k}}^G{p} +~ _{B_{k}}^G{v}\varDelta{t} + \iint_{[t_k, t_{k+1}]} a_t ~ dt^2 \\
 _{B_{k+1}}^G{v} &amp;amp;=~ _{B_{k}}^G{v} + \int_{[t_k, t_{k+1}]} a_t ~ dt \\
 _{B_{k+1}}^G{q} &amp;amp;=~ _{B_{k}}^G{q} \otimes \int_{[t_k, t_{k+1}]} \frac{1}{2} ~ ^{B_{k}}{q} \otimes 
 \begin{bmatrix}
 \omega_t \\
 0
 \end{bmatrix} dt 
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
 _G^{B_k}{R} ~ _{B_{k+1}}^G{p} &amp;amp;=~ _{B_{k}}^G{p} +~ _{B_{k}}^G{v}\varDelta{t} + \iint_{[t_k, t_{k+1}]} a_t ~ dt^2 \\
 _G^{B_k}{R} ~ _{B_{k+1}}^G{v} &amp;amp;=~ _{B_{k}}^G{v} + \int_{[t_k, t_{k+1}]} a_t ~ dt \\
 _G^{B_k}{q} \otimes _{B_{k+1}}^G{q} &amp;amp;=~ \gamma_{B_{k+1}}^{B_k}
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
 \alpha_{B_{k+1}}^{B_k} &amp;amp;= \iint_{[t_k, t_{k+1}]} {_{B_t}^{B_k}{R}(\^a_t-b_{a_t}-n_a)} ~ dt^2 \\
 \beta_{B_{k+1}}^{B_k} &amp;amp;= \int_{[t_k, t_{k+1}]} {_{B_t}^{B_k}{R}(\^a_t-b_{a_t}-n_a)} ~ dt \\
 \gamma_{B_{k+1}}^{B_k} &amp;amp;= \int_{[t_k, t_{k+1}]} {\frac{1}{2}\Omega(\~\omega_t - b_{\omega_t} - n_\omega)\gamma_{B_t}^{B_{k+1}}} dt 
\end{align}
&lt;/code>&lt;/pre>&lt;pre tabindex="0">&lt;code class="language-math" data-lang="math">\begin{align}
 \alpha_{B_{k+1}}^{B_k} &amp;amp;\approx \^\alpha_{B_{k+1}}^{B_k} + J_{b_a}^\alpha\delta{b_a} + J_{b_a}^\alpha\delta{b_\omega}\\
 \beta_{B_{k+1}}^{B_k} &amp;amp;\approx \^\beta_{B_{k+1}}^{B_k} + J_{b_a}^\beta\delta{b_a} + J_{b_a}^\beta\delta{b_\omega}\\
 \gamma_{B_{k+1}}^{B_k} &amp;amp;\approx \^\gamma_{B_{k+1}}^{B_k} \otimes \begin{bmatrix}
 1 \\
 \frac{1}{2}J_{b_\omega}^\gamma\delta{b_\omega}
 \end{bmatrix} 
\end{align}
&lt;/code>&lt;/pre>&lt;h3 id="question-purpose-of-pre-integration">Question: Purpose of pre-integration?&lt;/h3></description></item><item><title/><link>https://bobblelaw.github.io/topics/slam/vio/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://bobblelaw.github.io/topics/slam/vio/</guid><description>&lt;h2 id="fej-explaination">FEJ Explaination&lt;/h2>
&lt;p>可观性问题会直接带来多传感器融合融态中的关键手段：FEJ First Estimated Jacobian.&lt;/p>
&lt;p>即不同残差对同一状态求Jacobian时，线性化点必须一致，以避免零空间null space退化而使不可观变量变可观。&lt;/p>
&lt;p>因此对其的理解是一件非常重要的事情。初学者在没有足够资料与文档解释的情况下，对其的理解是比较困难的。&lt;/p>
&lt;p>从业者应该都清楚，状态估计走到今天，前端已经进入了大成熟阶段，回环手段也较为一致(如word bag或语义)，后端优化是核心中的核心。而其中对于H矩阵/或信息矩阵的处理与维护是核心&lt;/p>
&lt;p>这类矩阵维护中，基于BA的滑动窗口滤波是基础手段之一，我们需要进行边缘化的同时传递先验信息(形成pose graph)，节约开销的同时能够将先验信息传递至下一个滑动窗口。&lt;/p>
&lt;p>现在我们将被Marginized后的变量描述为Xm，剩余的变量记为Xr&lt;/p>
&lt;p>Marginized后：先验信息经过舒尔补操作后分别由残差矩阵b与先验信息矩阵Λ构成&lt;/p>
&lt;p>这2个矩阵都由2个部分构成(具体构成就不写了)，对应的Jacobian和残差r需要由这2个矩阵通过正定方程(增量方程)反解出来&lt;/p>
&lt;p>但这样就出现了2个核心问题：&lt;/p>
&lt;ol>
&lt;li>被Marg后的变量与测量已经丢弃，先验信息矩阵Λ中关于Xr的Jacobian在后续求解中已无法更新&lt;/li>
&lt;li>Xr中部分变量还和其他残差有关，这些残差的Jacobian会随新的状态估计的迭代更新而不断在最新的线性化点中计算&lt;/li>
&lt;/ol>
&lt;p>则Λ变为了Λ1(线性化点固定)与Λ2（线性化点在变），这样最终将导致Λ的零空间改变，引入错误信息。这里导入线性代数的一个基础概念：满秩必可逆，转置逆不变，不满秩则有零空间。&lt;/p>
&lt;p>因此导入了FEJ&lt;/p>
&lt;p>可观性/能观性用最通俗和直观的方式来描述就是：状态一变，测量就变&lt;/p>
&lt;p>单目SLAM存在7自由度不可观：3旋转，3平移，尺度皆不可观&lt;br>
单目+IMU存在4自由度不可观：Yaw与3平移，pitch与roll因重力而可观，尺度因加速度计而可观&lt;/p>
&lt;p>因此大家在调试对应slam系统中，如果出现问题，一定要考虑到可观性/能观性问题。&lt;/p>
&lt;p>另外FEL在实际工程中也存在不少问题，待后续具体实验中详解&lt;/p>
&lt;p>Li Mingyang &amp;amp;&amp;amp; Huang Guoquan paper&lt;/p>
&lt;h2 id="where-the-drift-comes-from">Where the drift comes from?&lt;/h2>
&lt;p>整个VINS-MONO系统，较容易在系统静止或外力给予较大冲击时产生轨迹漂移，原因是imu的bias在预积分中持续发散，视觉重投影误差产生的约束失效(如静止)，先验约束可能会在LM的线性求解器中产生无法收敛的情况，导致后端优化完全失效，因视觉静止系统也无法回环。&lt;/p>
&lt;p>对这一类问题的定位方向如下&lt;/p>
&lt;ol>
&lt;li>标定方向：VINS-MONO对系统标定的要求很高，尤其是imu的内参标定如果不准确还会直接影响双传感器之间的外参，所以定位重心首先应该放在标定的准确性上。另外也应该检验可见光相机的标定准确性，可通过ORB-SLAM等进行验证&lt;/li>
&lt;li>对时方向，IMU与相机的对时是一个难度较大的事项，如果有MCU从传感器直读数据进行对时的话，会是一个比较好的方式，这样对应的td会比较小。但如果通过vins-mono自己的优化对时的话，实际是异步的方式，比较容易受到各种因素的干扰，如视觉数据通过网络输入产生的延时，通过VI数据接口进入主控中的vins系统处理也会有延时，这个问题目前我们还在处理当中&lt;/li>
&lt;li>imu本身的数据问题导致权重变化，使视觉约束无法再拉回imu的偏移，这个是之前定位问题的思路，但是实际位置姿态的估计是由3种核心约束构成的，后续定位过程中我们发现了先验约束是直接进入了无法迭代的状态，即在舒尔补或cholesky分解的过程就出现了严重问题，这个问题的定位需要更多的打印日志与数据分析，我们暂时排除了这种可能性将更多的精力放到了lamda数据NAN值上。为何在静止状态时视觉约束无法将imu轨迹漂移拉回，这个也是需要深度定位的问题，后续在问题解决后我们会给出分析与对应的解决手段.&lt;/li>
&lt;/ol>
&lt;h2 id="gaussian-distribution">Gaussian distribution&lt;/h2>
&lt;p>高翔博士的VSLAM十四讲是一本非常生动和工程的书籍，有助于初学者快速掌握相关的知识。但如果仅从14讲就切入行业的话，实际的困难有2方面，一方面是在真正的工程和商业落地中，硬件与PCBA的总体设计/供应链/大厂/方案商/生产商/商业竞争会带来一系列的问题(后续文章中会时不时提到)。另一方面是十四讲本身实在是写得太好和简单，容易让同学们自认为这个学科的简单快速的跳入工程，以忽略数理和基础知识的重要性。&lt;/p>
&lt;p>进入正题(理解如下部分与系列文章的后续部分需要较为扎实的线性代数基础，对概率导论中的Bayes法则清晰，熟悉高等数学中的求导/积分与链式法则)：整个讲解会以非常通俗甚至偏低智的方式完成，其目的是让大家能更简单的了解VSLAM与机器人状态估计领域并提起兴趣，可能会存在不少错误欢迎大家斧正。因为这个学科以及后续的多传感器融合融态的本质学习曲线确实是相当的陡峭。&lt;/p>
&lt;h3 id="高斯分布的2个重要概念">高斯分布的2个重要概念&lt;/h3>
&lt;p>expectation and variance. 分别对应到整个高斯在概率密度函数上的均值，在高斯分布上我们也可以将其理解为极小值或极大值，以及对应高斯分布这座小山的陡度或者斜率。&lt;/p>
&lt;p>简单带一嘴全概率公理：整个事件概率终值为1，也可以理解为概率密度/概率密度函数设值为1(其实是完全不同的概念，有兴趣的话可阅读MIT的概率导论)，可以理解为在x/y2轴平面图上通过积分反解出的面积。概率密度的零阶矩正好是整个全事件的概率个人理解的是，高斯分布在机器人状态估计中在大量应用，其期望与协方差的本质可以确定为状态估计本身的2种输入，分别以概率一阶矩与概率二阶矩来进行表述(对应泰勒展开)，直接对应于期望与方差/协方差(这2个其实是有差异的，但是初学者无需过多理会)高斯概率密度函数中的均值μ的英文表述为mean，与高斯分布的期望值是不同的概念&lt;/p>
&lt;h3 id="linear-and-non-linear">Linear and non-linear&lt;/h3>
&lt;p>实际机器人运动中所有涉及的状态估计均为非线性&lt;/p>
&lt;p>线性比较好理解，如在一个完全无风的篮球馆进行投篮触框之前的曲线(仅受重力影响，不要纠结地球自转行星引力了)，在一个材质完全一样的表面(又是一种假设)对底部材质完全一样的质量块给予一个推力带来的运动到停止的过程。&lt;/p>
&lt;p>从如上的2个小例子大家就会明白，纯粹的线性运动，实际几乎是不可能存在的理想状态。&lt;/p>
&lt;p>但凡事都有2个方面，极端非线性运动的情况，其实在机器人实际运动和操作中也是稀少的。绝大部分的机器人状态估计，在工业与民用范畴，都偏向于轻度或中度非线性。&lt;/p>
&lt;h3 id="大量地将线性高斯系统假设进实际工程">大量地将线性高斯系统假设进实际工程&lt;/h3>
&lt;p>从1和2大家就能看出来，实际真实的VSLAM与多传感器融合融态工程中，就是将大量实际非线性运动的状态估计，以线性高斯系统模拟和假设出来，并作出最接近的状态估计。这里面最常用的数学工具就是矩阵与最小二乘，因为第一篇主要讨论高斯分布，就不在这赘述了。后续会详解的以卡尔曼滤波/扩展卡尔曼滤波进行的状态估计也会与这块相关。&lt;/p>
&lt;h3 id="统计独立性与不相关性">统计独立性与不相关性&lt;/h3>
&lt;p>在这里我们只需要牢记在高斯系统或高斯概率密度函数中，统计独立性和不相关性是等价的。其实这2种特性在概率导论中有清晰的描述，就不赘述了。&lt;/p>
&lt;h3 id="香农信息与互信息">香农信息与互信息&lt;/h3>
&lt;p>此处我们需要牢记，这2种信息对应的均为以定量来刻画不确定性，且香农信息和互信息可以实现换算关系。香农信息H以求概率密度函数负ln对数的期望值方式实现。(ln和exp也广泛应用在机器人状态估计中，涉及李群李代数部分详解)。互信息表述2个随机变量x与y之间的当一个已知后另一个的不确定性减少了多少。在联合高斯分布估计状态信息时广泛应用。香农信息与互信息具备换算关系。&lt;/p>
&lt;h3 id="克拉美罗下界crlb与fisher-information-matrix">克拉美罗下界(CRLB)与Fisher information matrix&lt;/h3>
&lt;p>又是2个听起来很难懂的术语？克拉美罗下界是一种方法，定义为：大家可以理解为一个界限，界定了一个参数的真实值的无偏估计(机器人状态估计本质)的协方差Σ，可以由费歇尔信息矩阵I来定义边界(机器人状态估计学习与工程中最容易搞错的就是符号和矩阵名称，不同的书籍体系中用的对应描述都有区别，一定要注意)。最终起的结果是利用了已有观测值估计参数效果和估计方式好坏的标准。此处有一重要的关系：如果我们从高斯概率密度函数中进行了K次采样(即传感器测量)，且均为统计独立，则其费歇尔信息矩阵正好是高斯协方差Σ逆的K倍。其CRLB大于等于1/K*Σ。表明在CRLB处我们状态估计量的期望等于高斯概率密度函数的均值μ通俗说就是：在高斯概率密度函数下，状态估计值的不确定下界，随测量/观测值增加变得越来越小，正符合我们的要求与标准。&lt;/p>
&lt;h3 id="为何要使用联合高斯分布与联合高斯概率密度函数">为何要使用联合高斯分布与联合高斯概率密度函数？&lt;/h3>
&lt;p>这里有一个和工程紧密结合的通俗直观的解释：在机器人状态估计的过程中，我们需要得到的状态估计至少涉及空间中的2种关键变量：相机位置姿态C与路标位置姿态L，这样我们才能在完成相机位置姿态估计的同时完成建图。因为坐标系实际都是相对的。另外我们的测量值在多传感器融合融态中也有多种观测与测量，比如相机给予的观测值与视觉重投影误差，陀螺仪给予的测量值，轮速计或GPS/RTK给予的位置姿态测量值。 因此大量的使用联合高斯分布与联合高斯概率密度是常态。如经典卡尔曼的基础形式：旧的估计综合预测+测量，导入卡尔曼增益与革新量，输出新的估计。此处还需要注意归一化积，这是个很简单的概念就不赘述了，就是把多个高斯分布用某种数学形式加入一个归一化常数进行归一。大家只需要知道多个pdf(高斯概率密度函数)归一化后的结果仍然是pdf即可&lt;/p></description></item></channel></rss>