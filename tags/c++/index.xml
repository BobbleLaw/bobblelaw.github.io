<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>C++ on Bobble Law</title><link>/tags/c++/</link><description>Recent content in C++ on Bobble Law</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy; 2022 &lt;a href="#">Bobble Law&lt;/a> and &lt;a href="#">Stay Inc.&lt;/a></copyright><lastBuildDate>Sat, 23 Oct 2021 02:00:00 +0000</lastBuildDate><atom:link href="/tags/c++/index.xml" rel="self" type="application/rss+xml"/><item><title>Moves in Returns</title><link>/posts/moves-in-return/</link><pubDate>Sat, 23 Oct 2021 02:00:00 +0000</pubDate><guid>/posts/moves-in-return/</guid><description>Today we&amp;rsquo;ll discuss code of the form:
T work(/* ... */) { /* ... */ return x; } This is a classical &amp;ldquo;return-by-value&amp;rdquo; and (wrongfully) associated with copies and overhead.
In many cases, this will actually move the result instead of copying it. For modern C++, one could even argue that this will move in most cases (or, as we will see, completely elide the copy and directly construct in the result memory).
This post discusses several common patterns and if they are moved, copies, or elided.</description></item><item><title>Optimization without Inlining</title><link>/posts/inline-optimization/</link><pubDate>Sun, 17 Oct 2021 02:00:00 +0000</pubDate><guid>/posts/inline-optimization/</guid><description>Inlining is one of the most important compiler optimizations. We can often write abstractions and thin wrapper functions without incurring any performance penalty, because the compiler will expand the method for us at call site.
If a function is not inlined, conventional wisdom says that the compiler has to assume that the method can modify any global state and change the memory behind any pointer or reference that might have &amp;ldquo;escaped&amp;rdquo;.
In this short post, I&amp;rsquo;ll demonstrate exactly this effect.</description></item><item><title>std::unordered_map Performance and Usage</title><link>/posts/unordered-map-usage-and-performance/</link><pubDate>Mon, 03 May 2021 00:00:00 +0000</pubDate><guid>/posts/unordered-map-usage-and-performance/</guid><description>Origin Story We have always been told that hashmap is the best associative container that offer O(1) insert, delete, and lookup. However, the customization space of it is quite large and depending on the use case, the trade-off space can change radically. std::unordered_map is (in)famous for having an API that basically forces implementers to use &amp;ldquo;buckets with linked lists&amp;rdquo;, also known as separate chaining. Many performance-critical applications swear on open addressing, often storing keys and values directly in arrays (either together or separate).</description></item><item><title>std::sort multiple ranges</title><link>/posts/sort-multiple-range/</link><pubDate>Sat, 28 Nov 2020 02:00:00 +0000</pubDate><guid>/posts/sort-multiple-range/</guid><description>std::sort is a great utility. You can easily sort subranges and provide custom comparison functions. However, it struggles with the following scenario:
std::vector&amp;lt;int&amp;gt; keys = ...; std::vector&amp;lt;std::string&amp;gt; values = ...; std::sort(...); // ??? We want to sort by keys but keep the 1-on-1 correspondence with values, i.e. keep the ranges &amp;ldquo;in sync&amp;rdquo; during sorting. A common solution is to allocate a vector of indices, sort these indices, and then apply the resulting permutation. However, the need for an additional allocation and bad cache locality due to indirection make this a suboptimal solution.</description></item><item><title>Multi-Level Break in C++ via IIFE</title><link>/posts/multi-level-break/</link><pubDate>Wed, 28 Oct 2020 02:00:00 +0000</pubDate><guid>/posts/multi-level-break/</guid><description>I guess we all have been at this point.
for (auto i : ...) for (auto j : ...) if (condition(i, j)) { break outer??? } You want to search something, and for one reason or another you end up with a nested loop. You find what you searched for and now want to break all the way to the outer loop.
If only we had multi-level breaks.
But we don&amp;rsquo;t.
So people introduce flags:</description></item><item><title>range_ref&lt;T></title><link>/posts/range-ref/</link><pubDate>Sat, 24 Oct 2020 02:00:00 +0000</pubDate><guid>/posts/range-ref/</guid><description>Passing references to functions is great.
struct some_user_type; void foo(some_user_type const&amp;amp; v) { // freely read from v } Memory management and lifetime handling is done by the caller. Users of your function / API have a liberating amount of freedom how they organize their data: on the stack, on the heap, in smart pointers, in vectors, it doesn&amp;rsquo;t matter. They can pass a reference to your function. No (potentially expensive) copy is performed.
From an API perspective, C++ references are views on a single object.</description></item><item><title>Recursive Lambdas in C++</title><link>/posts/recursive-lambda-function/</link><pubDate>Sat, 12 Sep 2020 02:00:00 +0000</pubDate><guid>/posts/recursive-lambda-function/</guid><description>auto fib = [](int n) { if (n &amp;lt;= 1) return n; return fib(n - 1) + fib(n - 2); }; auto i = fib(7); If only it were that simple.
Obviously, any performance-conscious programmer will compute Fibonacci numbers iteratively (or even explicitly), but this solution will serve as an example for an underappreciated tool: recursive lambdas.
Lambdas are one of my favorite features in any programming language and while I long for a shorter syntax in C++, I still use them quite ubiquitously, especially for local functions.</description></item><item><title>Consider deleting your rvalue ref-qualified assignment operators</title><link>/posts/delete-rvalue-ref-assignment/</link><pubDate>Sun, 22 Sep 2019 08:00:00 +0000</pubDate><guid>/posts/delete-rvalue-ref-assignment/</guid><description>The title might sound like an incantation to summon some mid-tier C++ god but it addresses a very real everyday pitfall:
struct foo { ... }; foo get_my_foo() { ... } // .. some code later: foo f; get_my_foo() = f; This compiles &amp;hellip; and does nothing useful.
We&amp;rsquo;ve assigned f to a temporary foo. No error, no warning.
A Real-Life Example In the math library I&amp;rsquo;m writing we have a mat struct for matrices and vec for vectors.</description></item><item><title>Basic Floating Point Optimizations</title><link>/posts/floating-point-optimizations/</link><pubDate>Fri, 09 Aug 2019 01:00:00 +0000</pubDate><guid>/posts/floating-point-optimizations/</guid><description>Ever seen some people write f * 0.5 when they mean f / 2?
Or if the compiler is able to optimize the f * 1.0 that you added for clarity?
Maybe you wrote f + f instead of f * 2 as a clever optimization?
Modern compilers are basically magic, but do they actually perform these optimizations? And, more importantly, why is f + 0.0 slower than f - 0.0?
Preliminaries Many posts have been written about elaborate magic involving IEEE 754 floating point numbers.</description></item></channel></rss>