<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Optimization on Bobble Law</title><link>https://bobblelaw.github.io/tags/optimization/</link><description>Recent content in Optimization on Bobble Law</description><generator>Hugo</generator><language>en-US</language><lastBuildDate>Wed, 17 Mar 2021 20:47:18 +0800</lastBuildDate><atom:link href="https://bobblelaw.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Automatic Differentiation</title><link>https://bobblelaw.github.io/posts/understanding-automatic-differentiation/</link><pubDate>Wed, 17 Mar 2021 20:47:18 +0800</pubDate><guid>https://bobblelaw.github.io/posts/understanding-automatic-differentiation/</guid><description>&lt;p>Deriving derivatives is not fun. In this post, I will deep dive into the methods for automatic differentiation (AD). After reading this post, I hope you can feel confident with using the various AD techniques, and hopefully never manually calculate derivatives again. Note that this post is not a comparison between AD libraries. For that, a good starting point is &lt;a href="">here&lt;/a>.&lt;/p>
&lt;h2 id="why-automatic-differentiation">Why Automatic Differentiation?&lt;/h2>
&lt;p>Automatic differentiation is a natural continuation of scientists and engineers’ pursuit for mechanizing computation. After all, we learn how to take derivatives by memorizing a set of rules. Why can’t computers do the same thing?&lt;/p></description></item></channel></rss>