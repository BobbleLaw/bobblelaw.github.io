<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>optimization on Bobble Law</title><link>https://bobblelaw.github.io/tags/optimization/</link><description>Recent content in optimization on Bobble Law</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy; 2022 &lt;a href="#">Bobble Law&lt;/a> and &lt;a href="#">Stay Inc.&lt;/a></copyright><lastBuildDate>Wed, 17 Mar 2021 20:47:18 +0800</lastBuildDate><atom:link href="https://bobblelaw.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Automatic Differentiation</title><link>https://bobblelaw.github.io/posts/understanding-automatic-differentiation/</link><pubDate>Wed, 17 Mar 2021 20:47:18 +0800</pubDate><guid>https://bobblelaw.github.io/posts/understanding-automatic-differentiation/</guid><description>Deriving derivatives is not fun. In this post, I will deep dive into the methods for automatic differentiation (AD). After reading this post, I hope you can feel confident with using the various AD techniques, and hopefully never manually calculate derivatives again. Note that this post is not a comparison between AD libraries. For that, a good starting point is here.
Why Automatic Differentiation? Automatic differentiation is a natural continuation of scientists and engineersâ€™ pursuit for mechanizing computation.</description></item></channel></rss>